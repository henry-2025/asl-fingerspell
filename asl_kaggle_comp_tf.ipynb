{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5471,
     "status": "ok",
     "timestamp": 1692377945068,
     "user": {
      "displayName": "Henry Pick",
      "userId": "08619058192611124262"
     },
     "user_tz": 420
    },
    "id": "mPMvSYMCBEVa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prqYjNncArsQ"
   },
   "source": [
    "## Data Pipeline\n",
    "We already have the dataset stored in compressed binary records in a gcloud bucket. Using the tf and gcloud apis, we can load these records directly into a dataset without having to use any auxiliary disk space on this notebook. I am trying out two methods here. The first is one emlpoyed by @hoyso48, the winner of the isolated hand sign competition. Idk how they were able to select significant coordinates (by hand?), but this mere fact made me skeptical that PCA would be more statistically robust.\n",
    "\n",
    "For PCA, I perform incremental PCA on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1692377987763,
     "user": {
      "displayName": "Henry Pick",
      "userId": "08619058192611124262"
     },
     "user_tz": 420
    },
    "id": "amDlteztijRW"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './aslf-5fold-20230815-001950/'\n",
    "DATASET_FILENAMES = list(map(lambda x: os.path.join(DATA_DIR, x), os.listdir(DATA_DIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1692377987764,
     "user": {
      "displayName": "Henry Pick",
      "userId": "08619058192611124262"
     },
     "user_tz": 420
    },
    "id": "KcI3E-yMBGwl"
   },
   "outputs": [],
   "source": [
    "def decode_features(b):\n",
    "    features = {\n",
    "    'frame': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    'sequence': tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "    }\n",
    "    decoded = tf.io.parse_single_example(b, features)\n",
    "    decoded['frame'] = tf.reshape(tf.io.decode_raw(decoded['frame'], out_type=np.float32), (-1, ROWS_PER_FRAME, 3))\n",
    "    decoded['sequence'] = tf.io.decode_raw(decoded['sequence'], out_type=np.int64)\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r'-([0-9]*)\\.').search(filename.split('/')[-1]).group(1)) for filename in filenames]\n",
    "    return sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 677,
     "status": "ok",
     "timestamp": 1692121267818,
     "user": {
      "displayName": "Henry Pick",
      "userId": "08619058192611124262"
     },
     "user_tz": 420
    },
    "id": "h9VgAYQhREDs",
    "outputId": "ebe409d0-ae02-4336-a687-75f54819625a"
   },
   "outputs": [],
   "source": [
    "## code copied from @hoyso48\n",
    "ROWS_PER_FRAME = 543\n",
    "MAX_FRAME_LEN = 384\n",
    "MAX_SEQ_LEN = 30\n",
    "NUM_CLASSES  = 59\n",
    "PAD = -100.\n",
    "NOSE=[\n",
    "    1,2,98,327\n",
    "]\n",
    "LNOSE = [98]\n",
    "RNOSE = [327]\n",
    "LIP = [ 0,\n",
    "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "LLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\n",
    "RLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n",
    "\n",
    "POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n",
    "LPOSE = [513,505,503,501]\n",
    "RPOSE = [512,504,502,500]\n",
    "\n",
    "REYE = [\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "]\n",
    "LEYE = [\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    466, 388, 387, 386, 385, 384, 398,\n",
    "]\n",
    "\n",
    "LHAND = np.arange(468, 489).tolist()\n",
    "RHAND = np.arange(522, 543).tolist()\n",
    "\n",
    "POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE #+POSE\n",
    "\n",
    "NUM_NODES = len(POINT_LANDMARKS)\n",
    "CHANNELS = 6*NUM_NODES\n",
    "\n",
    "print(NUM_NODES)\n",
    "\n",
    "# TODO: figure out why this is *\n",
    "print(CHANNELS)\n",
    "\n",
    "def interp1d_(x, target_len, method='random'):\n",
    "    length = tf.shape(x)[1]\n",
    "    target_len = tf.maximum(1,target_len)\n",
    "    if method == 'random':\n",
    "        if tf.random.uniform(()) < 0.33:\n",
    "            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bilinear')\n",
    "    else:\n",
    "        if tf.random.uniform(()) < 0.5:\n",
    "            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bicubic')\n",
    "        else:\n",
    "            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'nearest')\n",
    "    else:\n",
    "        x = tf.image.resize(x, (target_len,tf.shape(x)[1]),method)\n",
    "    return x\n",
    "\n",
    "def tf_nan_mean(x, axis=0, keepdims=False):\n",
    "      '''Compute the mean of a set of frames about an excluding the indices that are nan'''\n",
    "      return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
    "    '''Compute the std on an axis about a center excluding the indices that are nan'''\n",
    "    if center is None:\n",
    "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
    "    d = x - center\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
    "\n",
    "class Preprocess(tf.keras.layers.Layer):\n",
    "    '''Perform feature selection for frames, and normalization of the dataframe'''\n",
    "    def __init__(self, max_len=MAX_SEQ_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.point_landmarks = point_landmarks\n",
    "    \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # add a batch dimension to the inputs if it doesn't exist\n",
    "        if tf.rank(inputs) == 3:\n",
    "              x = inputs[None,...]\n",
    "        else:\n",
    "              x = inputs\n",
    "        \n",
    "        # find the mean about the nose (point 17) and interpolate to 0.5 where the mean cannot be calculated\n",
    "        mean = tf_nan_mean(tf.gather(x, [17], axis=2), axis=[1,2], keepdims=True)\n",
    "        mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n",
    "        \n",
    "        # extract the point landmarks that we have explicitly specified\n",
    "        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n",
    "        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
    "        \n",
    "        # normalize\n",
    "        x = (x - mean)/std\n",
    "        \n",
    "        # truncate to max_len\n",
    "        if self.max_len is not None:\n",
    "            x = x[:,:self.max_len]\n",
    "        length = tf.shape(x)[1]\n",
    "        \n",
    "        # discard z dim. Try without\n",
    "        x = x[...,:2]\n",
    "        \n",
    "        # first and second order rates of change as features\n",
    "        dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "        dx2 = tf.cond(tf.shape(x)[1]>2,lambda:tf.pad(x[:,2:] - x[:,:-2], [[0,0],[0,2],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "        \n",
    "        # stack all features along axis 3 and then concat along axis 3\n",
    "        x = tf.concat([\n",
    "          tf.reshape(x, (-1,length,2*len(self.point_landmarks))),\n",
    "          tf.reshape(dx, (-1,length,2*len(self.point_landmarks))),\n",
    "          tf.reshape(dx2, (-1,length,2*len(self.point_landmarks))),\n",
    "        ], axis = -1)\n",
    "        \n",
    "        # finally, sub zeros in nans\n",
    "        x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "elapsed": 2672,
     "status": "ok",
     "timestamp": 1692117615876,
     "user": {
      "displayName": "Henry Pick",
      "userId": "08619058192611124262"
     },
     "user_tz": 420
    },
    "id": "OA1PIdk2SocH",
    "outputId": "d1783c86-7995-4ea6-a472-7d6f7c5bf9bf"
   },
   "outputs": [],
   "source": [
    "# little dataset visualizer\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def filter_nans(frames):\n",
    "  '''Filter out all frames where all nan'''\n",
    "  return frames[~np.isnan(frames).all(axis=(-2,-1))]\n",
    "\n",
    "\n",
    "ds = tf.data.TFRecordDataset(DATASET_FILENAMES, compression_type='GZIP', name='aslf5folddataset', num_parallel_reads=tf.data.AUTOTUNE)\n",
    "ds = ds.map(decode_features)\n",
    "print(ds)\n",
    "\n",
    "\n",
    "# find the first example where the left hand is visible\n",
    "for x in ds:\n",
    "    temp = x['frame'].numpy()\n",
    "    if not len(filter_nans(temp[:,LHAND])) == 0:\n",
    "        break\n",
    "\n",
    "edges = [(0,1),(1,2),(2,3),(3,4),(0,5),(0,17),(5,6),(6,7),(7,8),(5,9),(9,10),(10,11),(11,12),\n",
    "         (9,13),(13,14),(14,15),(15,16),(13,17),(17,18),(18,19),(19,20)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def plot_frame(frame, edges=[], idxs=[]):\n",
    "\n",
    "    frame[np.isnan(frame)] = 0\n",
    "    x = list(frame[...,0])\n",
    "    y = list(frame[...,1])\n",
    "    if len(idxs) == 0:\n",
    "        idxs = list(range(len(x)))\n",
    "    ax.clear()\n",
    "    ax.scatter(x, y, color='dodgerblue')\n",
    "    for i in range(len(x)):\n",
    "        ax.text(x[i], y[i], idxs[i])\n",
    "\n",
    "    for edge in edges:\n",
    "        ax.plot([x[edge[0]], x[edge[1]]], [y[edge[0]], y[edge[1]]], color='salmon')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "def animate_frames(frames, edges=[], idxs=[]):\n",
    "    anim = FuncAnimation(fig, lambda frame: plot_frame(frame, edges, idxs), frames=frames, interval=100)\n",
    "    return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574,
     "output_embedded_package_id": "1W6lSg94u_NfPrdLzWcQyWH8-Hh196Lcu"
    },
    "executionInfo": {
     "elapsed": 67538,
     "status": "ok",
     "timestamp": 1692117689511,
     "user": {
      "displayName": "Henry Pick",
      "userId": "08619058192611124262"
     },
     "user_tz": 420
    },
    "id": "CPnLqtvzUD3S",
    "outputId": "7b562274-3948-4a5d-9a54-cabdc9224272"
   },
   "outputs": [],
   "source": [
    "# Left hand check\n",
    "animate_frames(filter_nans(temp[:,LHAND]),edges=edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04q3xgYYVfrw"
   },
   "outputs": [],
   "source": [
    "# PCA action\n",
    "N_PCA = 100\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "class PCAPreprocess(tf.keras.layers.Layer):\n",
    "  '''Preprocessing before feeding to incremental PCA'''\n",
    "  def __init__(self, max_len=MAX_LEN, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # add a batch dimension to the inputs if it doesn't exist\n",
    "    if tf.rank(inputs) == 3:\n",
    "      x = inputs[None,...]\n",
    "    else:\n",
    "      x = inputs\n",
    "\n",
    "    # find the mean about the nose (point 17) and interpolate to 0.5 where the mean cannot be calculated\n",
    "    mean = tf_nan_mean(tf.gather(x, [17], axis=2), axis=[1,2], keepdims=True)\n",
    "    mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n",
    "\n",
    "    # standardize\n",
    "    std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
    "    x = (x - mean)/std\n",
    "\n",
    "    if self.max_len is not None:\n",
    "      x = x[:,:self.max_len]\n",
    "    length = tf.shape(x)[1]\n",
    "    x = x[...,:2]\n",
    "\n",
    "    # first and second order rates of change as features\n",
    "    dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "    dx2 = tf.cond(tf.shape(x)[1]>2,lambda:tf.pad(x[:,2:] - x[:,:-2], [[0,0],[0,2],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "\n",
    "    # create a new axis for the derivative features\n",
    "    x = tf.concat([\n",
    "      tf.reshape(x, (-1,length,2*len(self.point_landmarks))),\n",
    "      tf.reshape(dx, (-1,length,2*len(self.point_landmarks))),\n",
    "      tf.reshape(dx2, (-1,length,2*len(self.point_landmarks))),\n",
    "    ], axis = -1)\n",
    "\n",
    "    # sub zeros in nans\n",
    "    x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
    "    # flatten along the feature, coordinate, and derivative features\n",
    "    b, s, f, c, d = x.get_shape()\n",
    "    x = tf.reshape(x, (b,s,f*c*d))\n",
    "    return x\n",
    "\n",
    "def do_ipca(ds, max_len=MAX_FRAME_LEN, n_components=N_PCA):\n",
    "  preprocess = PCAPreprocess(max_len)\n",
    "  ds = ds.map(lambda x: preprocess(x), tf.data.AUTOTUNE)\n",
    "\n",
    "  pca = IncrementalPCA(n_components)\n",
    "  pca.fit(ds)\n",
    "\n",
    "\n",
    "class PCAApplication(tf.keras.layers.Layer):\n",
    "  '''Apply PCA'''\n",
    "  def __init__(self, pca, max_len=MAX_LEN, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.pca = pca\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def call(self,inputs):\n",
    "    if tf.rank(inputs) == 3:\n",
    "      x = inputs[None,...]\n",
    "    else:\n",
    "      x = inputs\n",
    "\n",
    "    x = pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "error",
     "timestamp": 1692057546409,
     "user": {
      "displayName": "Henry Pick",
      "userId": "08619058192611124262"
     },
     "user_tz": 420
    },
    "id": "_LrLZxped63B",
    "outputId": "64a1667d-677b-461f-f100-1167666118f6"
   },
   "outputs": [],
   "source": [
    "do_ipca(ds, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BABO8hZv-4ro"
   },
   "source": [
    "## Augmentation\n",
    "\n",
    "Consider some of the following:\n",
    "- Flips\n",
    "- Rescalings\n",
    "- random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ny7-OPQ_t4x"
   },
   "source": [
    "# Data Pipeline Todos\n",
    "- [x] come up with a more efficient encoding format to be read\n",
    "- [x] Make sure that the data is not corrupted *before* you upload 60G of this to s3 lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOFj83pPAL9H"
   },
   "source": [
    "## Data Augmentation\n",
    "Some operations performed to vary training data in ways that may generalize the training distributions based on the data that we currently have. Some operations that we perform:\n",
    "\n",
    "- Flipping\n",
    "- Random resampling\n",
    "- Random affine transformation in space\n",
    "- Temporal masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "executionInfo": {
     "elapsed": 3275,
     "status": "ok",
     "timestamp": 1692124533618,
     "user": {
      "displayName": "Henry Pick",
      "userId": "08619058192611124262"
     },
     "user_tz": 420
    },
    "id": "y5kUA9SyFMfe",
    "outputId": "a1e4036c-e212-4fed-db97-ae93bf295484"
   },
   "outputs": [],
   "source": [
    "def data_augmentation(x):\n",
    "  '''Data augmentation collection. Apply each transformation with the following probabilities\n",
    "\n",
    "  TODO: implement these transformations\n",
    "  '''\n",
    "  return x\n",
    "\n",
    "\n",
    "def filter_nans_tf(x, ref_point=POINT_LANDMARKS):\n",
    "  '''Apply the same operation as filter_nans for tensors'''\n",
    "  mask = tf.math.logical_not(tf.reduce_all(tf.math.is_nan(tf.gather(x,ref_point,axis=1)), axis=[-2,-1]))\n",
    "  x = tf.boolean_mask(x, mask, axis=0)\n",
    "  return x\n",
    "\n",
    "# TODO: see if we can get rid of the batch dimension in the preprocessing\n",
    "def preprocess(example, feature_preprocessor: Preprocess, augment=False, max_frame_len=MAX_FRAME_LEN, max_seq_len=MAX_SEQ_LEN):\n",
    "  '''Apply feature preprocessing to the feature, conditionally augment, return the resulting feature, label frame'''\n",
    "  x = example['frame']\n",
    "  x = filter_nans_tf(x)\n",
    "  if augment:\n",
    "    x = data_augmentation(x, max_frame_len)\n",
    "\n",
    "  x = feature_preprocessor(x)[0]\n",
    "  y = example['sequence'][:max_seq_len]\n",
    "\n",
    "  return x, y, tf.shape(x)[0], len(y)\n",
    "\n",
    "\n",
    "def get_data_set(file_list, batch_size, max_frame_len, max_seq_len, drop_remainder=False, augment=False, shuffle=False, repeat=False):\n",
    "  ds = tf.data.TFRecordDataset(file_list, compression_type='GZIP', num_parallel_reads=tf.data.AUTOTUNE)\n",
    "  ds = ds.map(decode_features, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  feature_preprocessor = Preprocess(max_frame_len, POINT_LANDMARKS)\n",
    "  ds = ds.map(lambda x: preprocess(x, feature_preprocessor, augment, max_frame_len), tf.data.AUTOTUNE)\n",
    "\n",
    "  # repeats the dataset indefinitely\n",
    "  if repeat:\n",
    "    ds = ds.repeat()\n",
    "\n",
    "  # shuffle the dataset with buffer size shuffle\n",
    "  # when we shuffle we can disable deterministic loading\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(shuffle)\n",
    "    options = tf.data.Options()\n",
    "    options.deterministic = False\n",
    "    ds = ds.with_options(options)\n",
    "\n",
    "  if batch_size:\n",
    "    ds = ds.padded_batch(batch_size, padding_values=(tf.constant(PAD, dtype=tf.float32), tf.constant(0, dtype=tf.int64), None, None), padded_shapes=([max_frame_len, CHANNELS],[max_seq_len], [], []), drop_remainder=drop_remainder)\n",
    "\n",
    "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "ds = get_data_set(DATASET_FILENAMES, 100, 100, 30, False, False, True, False)\n",
    "\n",
    "for x in ds:\n",
    "  display(x[0].shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYZ-kzogAP4L"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "error",
     "timestamp": 1692142070007,
     "user": {
      "displayName": "Henry Pick",
      "userId": "08619058192611124262"
     },
     "user_tz": 420
    },
    "id": "2ZY8ssqyfeDk",
    "outputId": "2340bff4-6268-4083-a530-456ef7f6c7d7"
   },
   "outputs": [],
   "source": [
    "class ECA(tf.keras.layers.Layer):\n",
    "    '''An efficient channel attention mechanism as described in https://arxiv.org/abs/1910.03151'''\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.gp = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding='same', use_bias=False)\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        nn = self.gp(inputs)\n",
    "        nn = tf.expand_dims(nn, -1) # unsqueeze along last axis\n",
    "        nn = self.conv(nn)\n",
    "        nn = tf.squeeze(nn, -1)\n",
    "        nn = tf.nn.sigmoid(nn)\n",
    "        nn = nn[:,None,:]\n",
    "        return inputs * nn\n",
    "      \n",
    "\n",
    "\n",
    "class LateDropout(tf.keras.layers.Layer):\n",
    "    '''A dropout layer that applies after only start_step instances of training'''\n",
    "    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.rate = rate\n",
    "        self.start_step = start_step\n",
    "        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "        self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg, trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.cond(self._train_counter < self.start_step, lambda: inputs, lambda: self.dropout(inputs, training=training))\n",
    "        if training:\n",
    "            self._train_counter.assign_add(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "    '''Apply 1D convolution to the inputs with a causal padding. In cases with very low frame rates, this causal padding can become significant.'''\n",
    "    def __init__(self,\n",
    "                 kernel_size=17,\n",
    "                 dilation_rate=1,\n",
    "                 use_bias=False,\n",
    "                 depthwise_initializer='glorot_uniform',\n",
    "                 name='',\n",
    "                 **kwargs):\n",
    "      super().__init__(name=name, **kwargs)\n",
    "      # we pad the input with zeros that allow the convolution filters to process the first kernel_size - 1 samples independently. This is why we call it a 'causal' filter.\n",
    "      # we do this instead of using the\n",
    "      self.padding = tf.keras.layers.ZeroPadding1D(padding=(dilation_rate*(kernel_size - 1), 0), name=name + '_pad')\n",
    "      self.conv = tf.keras.layers.DepthwiseConv1D(kernel_size,\n",
    "                                                  padding='valid',\n",
    "                                                  depth_multiplier=1,\n",
    "                                                  dilation_rate=dilation_rate,\n",
    "                                                  use_bias=use_bias,\n",
    "                                                  depthwise_initializer=depthwise_initializer,\n",
    "                                                  name=name + '_dwconv')\n",
    "      self.supports_masking = True\n",
    "\n",
    "    def call(self, x):\n",
    "      x = self.padding(x)\n",
    "      x = self.conv(x)\n",
    "      return x\n",
    "\n",
    "def Conv1DBlock(channel_size,\n",
    "                kernel_size=17,\n",
    "                dilation_rate=1,\n",
    "                drop_rate=0.0,\n",
    "                expand_ratio=2,\n",
    "                se_ratio=0.25,\n",
    "                activation='swish',\n",
    "                name=None):\n",
    "      '''Efficient 1D convolution block @hoyso48'''\n",
    "      if name == None:\n",
    "        name = str(tf.keras.backend.get_uid('mbblock'))\n",
    "\n",
    "      def apply(inputs):\n",
    "          channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
    "          channels_expand = channels_in * expand_ratio\n",
    "\n",
    "          skip = inputs\n",
    "          \n",
    "          x = tf.keras.layers.Dense(\n",
    "              channels_expand,\n",
    "              use_bias=True,\n",
    "              activation=activation,\n",
    "              name=name + '_expand_conv')(inputs)\n",
    "\n",
    "          x = CausalDWConv1D(kernel_size,\n",
    "                             dilation_rate=dilation_rate,\n",
    "                             use_bias=False,\n",
    "                             name=name + '_dwconv')(x)\n",
    "          x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
    "          x = ECA()(x)\n",
    "          \n",
    "          x = tf.keras.layers.Dense(\n",
    "              channel_size,\n",
    "              use_bias=True,\n",
    "              name=name + '_project_conv')(x)\n",
    "          if drop_rate > 0:\n",
    "              x = tf.keras.layers.Dropout(drop_rate,noise_shape=(None,1,1), name=name + '_drop')(x)\n",
    "          if (channels_in == channel_size):\n",
    "              x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
    "          return x\n",
    "      return apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_len=64, dropout_step=0, dim=192):\n",
    "    inp = tf.keras.Input((max_len, CHANNELS))\n",
    "    x = tf.keras.layers.Masking(mask_value=PAD, input_shape=(max_len,CHANNELS))(inp)\n",
    "    ksize=17\n",
    "    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n",
    "\n",
    "    x = Conv1DBlock(dim, ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim, ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim, ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim, ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim, ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim, ksize,drop_rate=0.2)(x)\n",
    "    x = tf.keras.layers.Dense(dim*2, activation=None, name='top_conv')(x)\n",
    "    x = LateDropout(0.8, start_step=dropout_step)(x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, name='classifier')(x)\n",
    "    return tf.keras.Model(inp, x)    \n",
    "model = get_model(384)\n",
    "\n",
    "# for x in ds:\n",
    "#     y = model(x[0])\n",
    "#     print(y.shape)\n",
    "#     break\n",
    "\n",
    "class EditDistanceMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalize=True,\n",
    "        dtype=\"float32\",\n",
    "        name=\"edit_distance\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
    "\n",
    "        if dtype not in {'float32', 'float64'}:\n",
    "            \n",
    "            raise ValueError(\n",
    "                \"`dtype` must be a floating point type. \"\n",
    "                f\"Received: dtype={dtype}\"\n",
    "            )\n",
    "\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self._aggregate_unnormalized_edit_distance = self.add_weight(\n",
    "            shape=(),\n",
    "            initializer=\"zeros\",\n",
    "            dtype=self.dtype,\n",
    "            name=\"aggregate_unnormalized_edit_distance\",\n",
    "        )\n",
    "        \n",
    "        if normalize:\n",
    "            self._aggregate_reference_length = self.add_weight(\n",
    "                shape=(),\n",
    "                initializer=\"zeros\",\n",
    "                dtype=self.dtype,\n",
    "                name=\"aggregate_reference_length\",\n",
    "            )\n",
    "        else:\n",
    "            self._number_of_samples = self.add_weight(\n",
    "                shape=(),\n",
    "                initializer=\"zeros\",\n",
    "                dtype=self.dtype,\n",
    "                name=\"number_of_samples\",\n",
    "            )\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        def validate_and_fix_rank(inputs, tensor_name):\n",
    "            if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor)):\n",
    "                inputs = tf.ragged.constant(inputs)\n",
    "\n",
    "            if inputs.shape.rank == 1:\n",
    "                return tf.RaggedTensor.from_tensor(inputs[tf.newaxis])\n",
    "            elif inputs.shape.rank == 2:\n",
    "                return inputs\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"{tensor_name} must be of rank 1 or 2. \"\n",
    "                    f\"Found rank: {inputs.shape.rank}\"\n",
    "                )\n",
    "\n",
    "        y_true = validate_and_fix_rank(y_true, \"y_true\")\n",
    "        y_pred = validate_and_fix_rank(y_pred, \"y_pred\")\n",
    "\n",
    "        if self.normalize:\n",
    "            self._aggregate_reference_length.assign_add(\n",
    "                tf.cast(tf.size(y_true.flat_values), dtype=self.dtype)\n",
    "            )\n",
    "\n",
    "        def calculate_edit_distance(args):\n",
    "            reference, hypothesis = args\n",
    "\n",
    "            reference = tf.sparse.from_dense([reference])\n",
    "            hypothesis = tf.sparse.from_dense([hypothesis])\n",
    "\n",
    "            edit_distance = tf.squeeze(\n",
    "                tf.edit_distance(\n",
    "                    hypothesis=hypothesis,\n",
    "                    truth=reference,\n",
    "                    normalize=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self._aggregate_unnormalized_edit_distance.assign_add(\n",
    "                tf.cast(edit_distance, dtype=self.dtype)\n",
    "            )\n",
    "            if not self.normalize:\n",
    "                self._number_of_samples.assign_add(tf.cast(1, dtype=self.dtype))\n",
    "            return 0\n",
    "\n",
    "        _ = tf.map_fn(\n",
    "            fn=calculate_edit_distance,\n",
    "            elems=(y_true, y_pred),\n",
    "            fn_output_signature=\"int8\",\n",
    "        )\n",
    "\n",
    "    def result(self):\n",
    "        if self.normalize:\n",
    "            if self._aggregate_reference_length == 0:\n",
    "                return 0.0\n",
    "            return (\n",
    "                self._aggregate_unnormalized_edit_distance\n",
    "                / self._aggregate_reference_length\n",
    "            )\n",
    "        if self._number_of_samples == 0:\n",
    "            return 0.0\n",
    "        return (\n",
    "            self._aggregate_unnormalized_edit_distance / self._number_of_samples\n",
    "        )\n",
    "\n",
    "    def reset_state(self):\n",
    "        self._aggregate_unnormalized_edit_distance.assign(0.0)\n",
    "        if self.normalize:\n",
    "            self._aggregate_reference_length.assign(0.0)\n",
    "        else:\n",
    "            self._number_of_samples.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"normalize\": self.normalize})\n",
    "        return config\n",
    "        \n",
    "\n",
    "class ASLFConvModel(tf.keras.Model):\n",
    "    def __init__(self, max_len=64, dropout_step=0, dim=192, beam_width=20, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.functional_layers = get_model(max_len, dropout_step, dim)\n",
    "        self.built = True\n",
    "        self.beam_width = beam_width\n",
    "        self.edit_dist = EditDistanceMetric(normalize=True)\n",
    "        \n",
    "    def train_step(self, example):\n",
    "        X, y, X_len, y_len = example\n",
    "\n",
    "        # keep track of all variables in computing teh loss with tf.GradientTape\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(X)\n",
    "            logits = tf.transpose(logits, perm=[1, 0, 2])\n",
    "            loss = tf.nn.ctc_loss(y, logits, y_len, X_len)\n",
    "\n",
    "        # get trainable variables and compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # update wegihts\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # update metrics\n",
    "        # Metrics are configured in `compile()`.\n",
    "        \n",
    "        return {'loss': tf.reduce_sum(loss)}\n",
    "                \n",
    "    def test_step(self, example):\n",
    "        X, y, X_len, y_len = example\n",
    "        batch_size, seq_len = y.shape\n",
    "        \n",
    "        logits = self(X)\n",
    "        logits = tf.transpose(logits, [1, 0, 2])\n",
    "        \n",
    "        loss = tf.nn.ctc_loss(y, logits, y_len, X_len)\n",
    "        paths, probs = tf.nn.ctc_beam_search_decoder(logits, X_len, beam_width=self.beam_width, top_paths=1)\n",
    "        pred = paths[0]\n",
    "\n",
    "        y = tf.RaggedTensor.from_tensor(y, y_len)\n",
    "        pred = tf.RaggedTensor.from_sparse(pred)\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            if metric.name == 'edit_distance':\n",
    "                metric.update_state(y, pred)\n",
    "\n",
    "        ret_metrics = {m.name: m.result() for m in self.metrics}\n",
    "        ret_metrics['loss'] = tf.reduce_sum(loss)\n",
    "        return ret_metrics\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [self.edit_dist]\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.functional_layers(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def get_strategy(device='TPU-VM'):\n",
    "    if 'TPU' in device:\n",
    "        tpu = 'local' if device == 'TPU-VM' else None\n",
    "        print('connecting to TPU...')\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu)\n",
    "        strategy = tf.distribut.TPUStrategy(tpu)\n",
    "        IS_TPU = True\n",
    "\n",
    "    if device == 'GPU' or device == 'CPU':\n",
    "        ngpu = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "        if ngpu>1:\n",
    "            print('Using multi GPU')\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "        elif ngpu==1:\n",
    "            print('Using single GPU')\n",
    "            strategy = tf.distribute.get_strategy()\n",
    "        else:\n",
    "            print('Using CPU')\n",
    "            strategy = tf.distribute.get_strategy()\n",
    "        IS_TPU = False\n",
    "\n",
    "    if device == 'GPU':\n",
    "        print('Num GPUs available: ', ngpu)\n",
    "\n",
    "    REPLICAS = strategy.num_replicas_in_sync\n",
    "    print('Replicas: ', REPLICAS)\n",
    "\n",
    "    return strategy, REPLICAS, IS_TPU\n",
    "\n",
    "STRATEGY, N_REPLICAS, IS_TPU = get_strategy('GPU')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "    def _save_model(self, epoch, batch, logs):\n",
    "        \"\"\"Saves the model.\n",
    "\n",
    "        Args:\n",
    "            epoch: the epoch this iteration is in.\n",
    "            batch: the batch this iteration is in. `None` if the `save_freq`\n",
    "              is set to `epoch`.\n",
    "            logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n",
    "        \"\"\"\n",
    "        logs = logs or {}\n",
    "\n",
    "        if (\n",
    "            isinstance(self.save_freq, int)\n",
    "            or self.epochs_since_last_save >= self.period\n",
    "        ):\n",
    "            # Block only when saving interval is reached.\n",
    "            self.epochs_since_last_save = 0\n",
    "            filepath = self._get_file_path(epoch, batch, logs)\n",
    "\n",
    "            # Create host directory if it doesn't exist.\n",
    "            dirname = os.path.dirname(filepath)\n",
    "            if dirname and not tf.io.gfile.exists(dirname):\n",
    "                tf.io.gfile.makedirs(dirname)\n",
    "\n",
    "            try:\n",
    "                if self.save_best_only:\n",
    "                    current = logs.get(self.monitor)\n",
    "                    if current is None:\n",
    "                        logging.warning(\n",
    "                            \"Can save best model only with %s available, \"\n",
    "                            \"skipping.\",\n",
    "                            self.monitor,\n",
    "                        )\n",
    "                    else:\n",
    "                        if self.monitor_op(current, self.best):\n",
    "                            if self.verbose > 0:\n",
    "                                io_utils.print_msg(\n",
    "                                    f\"\\nEpoch {epoch + 1}: {self.monitor} \"\n",
    "                                    \"improved \"\n",
    "                                    f\"from {self.best:.5f} to {current:.5f}, \"\n",
    "                                    f\"saving model to {filepath}\"\n",
    "                                )\n",
    "                            self.best = current\n",
    "                            if self.save_weights_only:\n",
    "                                self.model.save_weights(\n",
    "                                    filepath,\n",
    "                                    overwrite=True,\n",
    "                                    options=self._options,\n",
    "                                )\n",
    "                            else:\n",
    "                                self.model.functional_layers.save(\n",
    "                                    filepath,\n",
    "                                    overwrite=True,\n",
    "                                    options=self._options,\n",
    "                                )\n",
    "                        else:\n",
    "                            if self.verbose > 0:\n",
    "                                io_utils.print_msg(\n",
    "                                    f\"\\nEpoch {epoch + 1}: \"\n",
    "                                    f\"{self.monitor} did not improve \"\n",
    "                                    f\"from {self.best:.5f}\"\n",
    "                                )\n",
    "                else:\n",
    "                    if self.verbose > 0:\n",
    "                        io_utils.print_msg(\n",
    "                            f\"\\nEpoch {epoch + 1}: saving model to {filepath}\"\n",
    "                        )\n",
    "                    if self.save_weights_only:\n",
    "                        self.model.save_weights(\n",
    "                            filepath, overwrite=True, options=self._options\n",
    "                        )\n",
    "                    elif filepath.endswith(\".keras\"):\n",
    "                        self.model.save(filepath, overwrite=True)\n",
    "                    else:\n",
    "                        self.model.save(\n",
    "                            filepath, overwrite=True, options=self._options\n",
    "                        )\n",
    "\n",
    "                self._maybe_remove_file()\n",
    "            except IsADirectoryError:  # h5py 3.x\n",
    "                raise IOError(\n",
    "                    \"Please specify a non-directory filepath for \"\n",
    "                    \"ModelCheckpoint. Filepath used is an existing \"\n",
    "                    f\"directory: {filepath}\"\n",
    "                )\n",
    "            except IOError as e:  # h5py 2.x\n",
    "                # `e.errno` appears to be `None` so checking the content of\n",
    "                # `e.args[0]`.\n",
    "                if \"is a directory\" in str(e.args[0]).lower():\n",
    "                    raise IOError(\n",
    "                        \"Please specify a non-directory filepath for \"\n",
    "                        \"ModelCheckpoint. Filepath used is an existing \"\n",
    "                        f\"directory: f{filepath}\"\n",
    "                    )\n",
    "                # Re-throw the error for any other causes.\n",
    "                raise e\n",
    "\n",
    "def train_fold(CFG, fold, train_files, valid_files=None, strategy=STRATEGY, summary=True):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "\n",
    "    train_ds = get_data_set(train_files, batch_size=CFG.batch_size, max_frame_len=CFG.max_frame_len, max_seq_len=CFG.max_seq_len, drop_remainder=True, augment=CFG.augment, repeat=True)\n",
    "    if valid_files:\n",
    "        valid_ds = get_data_set(valid_files, batch_size=CFG.batch_size, max_frame_len=CFG.max_frame_len, max_seq_len=CFG.max_seq_len, drop_remainder=True, augment=CFG.augment, repeat=True)\n",
    "    else:\n",
    "        valid_files = []\n",
    "\n",
    "    num_train = count_data_items(train_files)\n",
    "    num_valid = count_data_items(valid_files)\n",
    "    steps_per_epoch = num_train//CFG.batch_size\n",
    "    with strategy.scope():\n",
    "        dropout_step = CFG.dropout_start_epoch*steps_per_epoch\n",
    "        model = ASLFConvModel(max_len=CFG.max_frame_len, dropout_step=dropout_step, dim=CFG.dim)\n",
    "        schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=CFG.learning_rate, decay_steps=steps_per_epoch, decay_rate=0.95)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=schedule)\n",
    "    \n",
    "        model.compile(\n",
    "            optimizer=opt,\n",
    "            steps_per_execution=steps_per_epoch,\n",
    "        )\n",
    "\n",
    "    if summary:\n",
    "        model.functional_layers.summary()\n",
    "        print()\n",
    "        print(train_ds, valid_ds)\n",
    "        print()\n",
    "\n",
    "    print(f'---------fold{fold}----------')\n",
    "    print(f'train:{num_train} valid:{num_valid}')\n",
    "    print()\n",
    "\n",
    "    if CFG.resume:\n",
    "        print('resume from epoch ', CFG.resume)\n",
    "        model.load_weights(f'{CFG.output_dir}/{CFG.train_name}-fold{fold}-last.weights')\n",
    "        if train_ds is not None:\n",
    "            model.evaluate(train_ds.take(steps_per_epoch))\n",
    "        if valid_ds is not None:\n",
    "            model.evaluate(valid_ds)\n",
    "            \n",
    "\n",
    "                                \n",
    "                            \n",
    "    \n",
    "    logger = tf.keras.callbacks.CSVLogger(f'{CFG.output_dir}/{CFG.train_name}-fold{fold}-logs.csv')\n",
    "    checkpoint = SubModelCheckpoint(f'{CFG.output_dir}/{CFG.train_name}-fold{fold}-best.h5', monitor='val_loss', verbose=0, save_best_only=True)\n",
    "    callbacks = []\n",
    "    if CFG.save_output:\n",
    "        callbacks.append(logger)\n",
    "        if fold != 'all':\n",
    "            callbacks.append(checkpoint)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.epoch-CFG.resume,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=valid_ds,\n",
    "        verbose=CFG.verbose,\n",
    "        validation_steps=num_valid//CFG.batch_size\n",
    "    )\n",
    "\n",
    "    if CFG.save_output:\n",
    "        try:\n",
    "            model.load_weights(f'{CFG.output_dir}/{CFG.train_name}-fold{fold}-best.h5')\n",
    "        except:\n",
    "            pass\n",
    "    if fold != 'all':\n",
    "        cv = model.evaluate(valid_ds, verbose=CFG.verbose, steps=num_valid//CFG.batch_size)\n",
    "    else:\n",
    "        cv = None\n",
    "    \n",
    "    return model, cv, history\n",
    "        \n",
    "\n",
    "def train_folds(CFG, folds, strategy=STRATEGY, summary=True):\n",
    "    \"\"\"Run training across folds. Setting a fold to 'all' will train for an epoch on the entire dataset\"\"\"\n",
    "    for fold in folds:\n",
    "        if fold != 'all':\n",
    "            all_files = DATASET_FILENAMES\n",
    "            train_files = [x for x in all_files if f'fold{fold}' not in x]\n",
    "            valid_files = [x for x in all_files if f'fold{fold}' in x]\n",
    "        else:\n",
    "            train_files = DATASET_FILENAMES\n",
    "            valid_files = None\n",
    "\n",
    "        train_fold(CFG, fold, train_files, valid_files, strategy=strategy, summary=summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    save_output = True\n",
    "    output_dir = './saves/'\n",
    "\n",
    "    seed = 42\n",
    "    verbose = 2 #0) silent 1) progress bar 2) one line per epoch\n",
    "\n",
    "    max_frame_len = 384\n",
    "    max_seq_len = 30\n",
    "    replicas = N_REPLICAS\n",
    "    learning_rate = 5e-4*replicas\n",
    "    lr_min = 1e-6\n",
    "    epoch = 1\n",
    "    warmup = 0\n",
    "    resume = 0\n",
    "    batch_size = 64 * replicas\n",
    "    augment = False\n",
    "    dropout_start_epoch = 0\n",
    "    dim = 192\n",
    "\n",
    "    train_name = f'aslf-fp32-192-8-seed{seed}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds(CFG, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and TFLite Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfPreprocess(tf.keras.layers.Layer):\n",
    "    '''Perform feature selection for frames, and normalization of the dataframe'''\n",
    "    def __init__(self, max_len=MAX_SEQ_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.point_landmarks = point_landmarks\n",
    "        self.permute = np.arange(0, 543*3)\n",
    "    \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # add a batch dimension to the inputs if it doesn't exist\n",
    "        if tf.rank(inputs) == 2:\n",
    "              x = inputs[None,...]\n",
    "        else:\n",
    "              x = inputs\n",
    "\n",
    "        # permute and then reshape to working format\n",
    "        x = tf.reshape(tf.gather(x, self.permute, axis=-1), (1, -1, 543, 3))\n",
    "        \n",
    "        # find the mean about the nose (point 17) and interpolate to 0.5 where the mean cannot be calculated\n",
    "        mean = tf_nan_mean(tf.gather(x, [17], axis=2), axis=[1,2], keepdims=True)\n",
    "        mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n",
    "        \n",
    "        # extract the point landmarks that we have explicitly specified\n",
    "        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n",
    "        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
    "        \n",
    "        # normalize\n",
    "        x = (x - mean)/std\n",
    "        \n",
    "        # truncate to max_len\n",
    "        if self.max_len is not None:\n",
    "            x = x[:,:self.max_len]\n",
    "        length = tf.shape(x)[1]\n",
    "        \n",
    "        # discard z dim. Try without\n",
    "        x = x[...,:2]\n",
    "        \n",
    "        # first and second order rates of change as features\n",
    "        dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "        dx2 = tf.cond(tf.shape(x)[1]>2,lambda:tf.pad(x[:,2:] - x[:,:-2], [[0,0],[0,2],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "        \n",
    "        # stack all features along axis 3 and then concat along axis 3\n",
    "        x = tf.concat([\n",
    "          tf.reshape(x, (-1,length,2*len(self.point_landmarks))),\n",
    "          tf.reshape(dx, (-1,length,2*len(self.point_landmarks))),\n",
    "          tf.reshape(dx2, (-1,length,2*len(self.point_landmarks))),\n",
    "        ], axis = -1)\n",
    "        \n",
    "        # finally, sub zeros in nans\n",
    "        x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
    "        return x\n",
    "\n",
    "class TFLiteModel(tf.Module):\n",
    "    \"\"\"\n",
    "    TensorFlow Lite model that takes input tensors and applies the preprocessing steps and then the ASLF model\n",
    "    \"\"\"\n",
    "    def __init__(self, weights_file, beam_width, CFG):\n",
    "        \"\"\"\n",
    "        Initialize the \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.prep_inputs = InfPreprocess()\n",
    "        self.beam_width = beam_width\n",
    "        self.model = get_model(CFG.max_frame_len, dim=CFG.dim)\n",
    "        self.model.load_weights(weights_file)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 543*3], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Applies the preprocessor and an ensemble of models to the inputs, selecting the most confident output\n",
    "        \"\"\"\n",
    "        x = self.prep_inputs(tf.cast(inputs, dtype=tf.float32))\n",
    "        x = self.model(x)\n",
    "        \n",
    "\n",
    "        # ctc beam decode\n",
    "        # outputs = tf.nn.ctc_beam_search_decoder(x, tf.shape(x), self.beam_width)\n",
    "        return x\n",
    "\n",
    "tflite_model = TFLiteModel('./saves/aslf-fp32-192-8-seed42-fold0-best.h5', 20, CFG)\n",
    "\n",
    "x = tflite_model(tf.random.normal((100, 543*3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMV6a2fFMa9nquHGU0Xa/YT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
