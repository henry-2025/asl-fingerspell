{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import asyncio\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count()\n",
    "\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## config stuff TODO: tidy this up\n",
    "n_splits = 5\n",
    "seed = 1\n",
    "CHUNK_SIZE = 512\n",
    "N_FEATURES = 543\n",
    "N_PART = 1\n",
    "part = 0\n",
    "\n",
    "n_procs = 4\n",
    "\n",
    "import json\n",
    "WORD_DICT = json.load(open('/kaggle/input/asl-fingerspelling/character_to_prediction_index.json'))\n",
    "manifest = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n",
    "manifest = manifest.groupby('file_id').sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "\n",
    "class ParquetReader():\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        \n",
    "    async def open_parquet(self):\n",
    "        self.parquet = pd.read_parquet(self.file_name)\n",
    "        self.examples = self.parquet.index.unique()\n",
    "        self.progress = tqdm(desc=f'reader {self.file_name}', total=len(self.examples))\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        frame = self.parquet.loc[idx]\n",
    "        self.progress.update(1)\n",
    "        \n",
    "        if type(frame) != pd.DataFrame:\n",
    "            return None\n",
    "        frame = frame.drop('frame', axis=1)\n",
    "        \n",
    "        frame = frame[sorted(frame.columns, key=lambda x: x[2:])]\n",
    "        n_frames = len(frame)\n",
    "        \n",
    "        return frame.values.reshape(n_frames, N_FEATURES, 3) # we shape this into a frame x feature x axis tensor\n",
    "    \n",
    "    def __contains__(self, idx):\n",
    "        return idx in self.examples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "class ChunkWriter():\n",
    "    def __init__(self, file_name, chunk_size):\n",
    "        self.file_name = file_name\n",
    "        self.num_written = 0\n",
    "        self.file = tf.io.TFRecordWriter(file_name, options='GZIP')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.progress = tqdm(desc=f'writer {file_name}', total=self.chunk_size)\n",
    "        \n",
    "    async def write(self, row, coordinates):\n",
    "        m_bytes = encode_row(row, coordinates)\n",
    "        self.file.write(m_bytes)\n",
    "        self.num_written += 1\n",
    "        self.progress.update(1)\n",
    "        return self\n",
    "    \n",
    "    def is_full(self):\n",
    "        return self.num_written >= self.chunk_size\n",
    "    \n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "        if self.num_written != self.chunk_size:\n",
    "            new_filename = re.sub(r\"(-)([0-9]*)(\\.)\",f\"\\g<1>{self.num_written}\\g<3>\", self.file_name)\n",
    "            os.rename(self.file_name, new_filename)\n",
    "\n",
    "class ParallelFoldWriterGroup():\n",
    "    \"\"\"A processing class that reads and writes tfrecord files in parallel. It is a little bit tedious because we don't have 1:1 correspondence between parquet files and training examples.\n",
    "    Yeah this might be a bit over-engineered. And yes, I know that it should use parallell processing instead of coroutines. That will be addressed when I am done with the project and have time to worry about these things.\"\"\"\n",
    "    def __init__(self, manifest, write_dir, n_folds, chunk_size, max_writers):\n",
    "        # generate fold groups\n",
    "        file_groups = [df for _, df in manifest.groupby('file_id')]\n",
    "        fold_size = len(manifest) // n_folds\n",
    "        random.shuffle(file_groups)\n",
    "        self.folds = pd.concat(file_groups).reset_index()\n",
    "        self.folds = [df.reset_index(drop=True) for _, df in self.folds.groupby(lambda x: x // fold_size)]\n",
    "        if len(self.folds) > n_folds:\n",
    "            self.folds[-1] = pd.concat(self.folds[-2:]).reset_index(drop=True)\n",
    "            del self.folds[-2]\n",
    "            \n",
    "        self.chunk_size = chunk_size\n",
    "        self.max_writers = min(fold_size // chunk_size, max_writers)\n",
    "        self.write_dir = write_dir\n",
    "        self.current_reader, self.next_reader = None, None\n",
    "        \n",
    "    async def main_write_loop(self):\n",
    "        for current_fold, fold in enumerate(self.folds):\n",
    "            current_chunk = 0\n",
    "#             fold = fold.iloc[:self.chunk_size*3]\n",
    "            read_files = fold.path.unique().tolist()[::-1]\n",
    "            read_files = [os.path.join('/kaggle/input/asl-fingerspelling/', x) for x in read_files]\n",
    "            examples_to_write = 0\n",
    "            await self.swap_readers(read_files, True)\n",
    "            done, pending, n_writers = set(), set(), 0\n",
    "            \n",
    "            for idx in fold.index:\n",
    "                # read row from the current parquet\n",
    "                row = fold.loc[idx]\n",
    "                if not row.sequence_id in self.current_reader:\n",
    "                    # current reader has expired. Need to swap to the new one\n",
    "                    await self.swap_readers(read_files)\n",
    "                    \n",
    "                # assert that we don't have this error that I am anticipating being a big issue\n",
    "                if not row.sequence_id in self.current_reader:\n",
    "                    raise RuntimeError('This should not happen since sequence ids are stored in the same order that the files are opened.')\n",
    "\n",
    "                coordinates = self.current_reader[row.sequence_id]\n",
    "                \n",
    "                # sometimes coordinates will be a single-row dataframe. In this case, just skip the row\n",
    "                if type(coordinates) != np.ndarray:\n",
    "                    continue\n",
    "                \n",
    "                # load the coordinates and row into one of the open readers\n",
    "                if n_writers < self.max_writers and len(fold) - examples_to_write > self.chunk_size:\n",
    "                    # we are filling up the writers pool\n",
    "                    writer = self.open_new_writer(current_fold, current_chunk)\n",
    "                    n_writers += 1\n",
    "                    current_chunk += 1\n",
    "                    examples_to_write += self.chunk_size\n",
    "                else:   \n",
    "                    new_done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "                    done = done.union(new_done)\n",
    "                    writer = done.pop().result()\n",
    "                    if writer.is_full() and len(fold) - examples_to_write < self.chunk_size:\n",
    "                        while done and writer.is_full():\n",
    "                            # we can write the entire fold with the currently-open writers, close the current writer\n",
    "                            writer.close()\n",
    "                            n_writers -= 1\n",
    "                            writer = done.pop().result()\n",
    "                        \n",
    "                    elif writer.is_full():\n",
    "                        # we can open a new writer\n",
    "                        writer.close()                        \n",
    "                        writer = self.open_new_writer(current_fold, current_chunk)\n",
    "                        examples_to_write += self.chunk_size\n",
    "                        current_chunk += 1\n",
    "\n",
    "                new_task = asyncio.create_task(writer.write(row, coordinates))\n",
    "                pending.add(new_task)\n",
    "                   \n",
    "            # close up shop. All pending writing tasks should finish\n",
    "            done, _ = await asyncio.wait(pending, return_when=asyncio.ALL_COMPLETED)\n",
    "            for writer in done:\n",
    "                writer = writer.result()\n",
    "                writer.close()\n",
    "                del writer\n",
    "                \n",
    "    async def swap_readers(self, files, new_fold=False):\n",
    "        \"\"\"Do this to keep a background reader that can be hot swapped with the current one when it is done\"\"\"\n",
    "        if new_fold:\n",
    "            if not self.current_reader:\n",
    "                assert not self.next_reader\n",
    "                self.current_reader = await ParquetReader(files.pop()).open_parquet()\n",
    "                if files:\n",
    "                    self.next_reader = ParquetReader(files.pop()).open_parquet()\n",
    "            else:\n",
    "                last_filename = self.current_reader.file_name\n",
    "                if last_filename in files:\n",
    "                    files.remove(last_filename)\n",
    "                if files:\n",
    "                    self.next_reader = ParquetReader(files.pop()).open_parquet()\n",
    "        else:\n",
    "            self.current_reader = await self.next_reader\n",
    "            if files:\n",
    "                self.next_reader = ParquetReader(files.pop()).open_parquet()\n",
    "            else:\n",
    "                self.next_reader = None                \n",
    "                \n",
    "    def open_new_writer(self, current_fold, current_chunk):\n",
    "        # logic for next chunk name\n",
    "        writer_name = os.path.join(self.write_dir, f'fold{current_fold}-{current_chunk}-{self.chunk_size}')\n",
    "        return ChunkWriter(writer_name, self.chunk_size)\n",
    "    \n",
    "def encode_row(row, coordinates):\n",
    "    \"\"\"Encode a row from the train manifest and the coordinates in a numpy array into a tfrecord bytes string\"\"\"\n",
    "    coordinates_encoded = coordinates.tobytes()\n",
    "    participant_id = int(row.participant_id)\n",
    "    sequence_id = int(row.sequence_id)\n",
    "    sequence = np.array(list(map(lambda x: WORD_DICT[x], row.phrase))).tobytes()\n",
    "    record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'coordinates': tf.train.Feature(bytes_list=tf.train.BytesList(value=[coordinates_encoded])),\n",
    "        'participant_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[participant_id])),\n",
    "        'sequence_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[sequence_id])),\n",
    "        'sequence': tf.train.Feature(bytes_list=tf.train.BytesList(value=[sequence]))})).SerializeToString()\n",
    "    return record_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/fold*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: move this to the colab notebook\n",
    "\n",
    "# # from IPython.display import HTML\n",
    "# import matplotlib.animation as animation\n",
    "# from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# def filter_nans(frames):\n",
    "#     return \n",
    "\n",
    "# def plot_frame(frame, edges=[], indices=[]):\n",
    "#     frame[np.isnan(frame)] = 0\n",
    "#     x = list(frame[...,0])\n",
    "#     y = list(frame[...,1])\n",
    "#     if len(indices) == 0:\n",
    "#         indices = list(range(len(x)))\n",
    "#     ax.clear()\n",
    "#     ax.scatter(x, y, color='blue')\n",
    "#     for i in range(len(x)):\n",
    "#         ax.text(x[i], y[i], indices[i])\n",
    "    \n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])\n",
    "#     ax.set_xticklabels([])\n",
    "#     ax.set_yticklabels([])\n",
    "    \n",
    "# def animate_frames(frames, edges=[], indices=[]):\n",
    "#         anim = FuncAnimation(fig, lambda frame: plot_frame(frame, edges, indices), frames=frames, interval=100)\n",
    "#         return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = f'ASLF-{n_splits}fold'\n",
    "\n",
    "os.makedirs(f'/tmp/{DATASET_NAME}', exist_ok=True)\n",
    "\n",
    "with open('/kaggle/input/kaggleapi/kaggle.json') as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "    \n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n",
    "\n",
    "!kaggle datasets init -p /tmp/{DATASET_NAME}\n",
    "\n",
    "with open(f'/tmp/{DATASET_NAME}/dataset-metadata.json') as f:\n",
    "    dataset_meta = json.load(f)\n",
    "\n",
    "dataset_meta['id'] = f'jonathanpick/{DATASET_NAME}'\n",
    "dataset_meta['title'] = DATASET_NAME\n",
    "\n",
    "with open(f'/tmp/{DATASET_NAME}/dataset-metadata.json', 'w') as output:\n",
    "    json.dump(dataset_meta, output)\n",
    "print(dataset_meta)\n",
    "\n",
    "!cp /tmp/{DATASET_NAME}/dataset-metadata.json /tmp/{DATASET_NAME}/meta.json\n",
    "!ls /tmp/{DATASET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeGroup = ParallelFoldWriterGroup(manifest, f'/tmp/{DATASET_NAME}', 5, 256, 10)\n",
    "await writeGroup.main_write_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "version_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "settings = {\n",
    "    'id': 'id',\n",
    "    'secret': 'secret'\n",
    "}\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=settings['id'],\n",
    "    aws_secret_access_key=settings['secret'],\n",
    "    region_name='us-west-1'\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = DATASET_NAME.lower() + \"-\" + version_name\n",
    "bucket = s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={\n",
    "                'LocationConstraint': 'us-west-1'\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_directory(bucket, directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            bucket.upload_file(os.path.join(root, file), file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_directory(bucket, f'/tmp/{DATASET_NAME}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
