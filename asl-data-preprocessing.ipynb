{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import itertools as it\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## config stuff\n",
    "DATA_DIR = '.'\n",
    "WORK_DIR = '.'\n",
    "CREATE_KAGGLE_DATASET = False\n",
    "N_FOLDS = 5\n",
    "CHUNK_SIZE = 256\n",
    "\n",
    "N_WORKERS = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_DICT = json.load(open(os.path.join(DATA_DIR, 'character_to_prediction_index.json')))\n",
    "\n",
    "\n",
    "def match_current_files(root_data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"matches examples in training to the files in the training directory\"\"\"\n",
    "    train = pd.read_csv(os.path.join(root_data_dir, 'train.csv'))\n",
    "\n",
    "    # create the list of files that we can parse\n",
    "    def parquet_match(x):\n",
    "        match = re.match(r'(\\d+)\\.parquet', x)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    file_ids = set(map(parquet_match, os.listdir(os.path.join(root_data_dir, 'train_landmarks'))))\n",
    "\n",
    "    return train[train.file_id.isin(file_ids)]\n",
    "\n",
    "manifest = match_current_files(DATA_DIR)\n",
    "manifest = manifest.groupby('file_id').sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = f'ASLF-{N_FOLDS}fold'\n",
    "DATASET_DIR = os.path.join(WORK_DIR, DATASET_NAME)\n",
    "\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "if CREATE_KAGGLE_DATASET:\n",
    "    with open('/kaggle/input/kaggleapi/kaggle.json') as f:\n",
    "        kaggle_creds = json.load(f)\n",
    "        \n",
    "    os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n",
    "    os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n",
    "    \n",
    "    !kaggle datasets init -p /tmp/{DATASET_NAME}\n",
    "    \n",
    "    with open(f'/tmp/{DATASET_NAME}/dataset-metadata.json') as f:\n",
    "        dataset_meta = json.load(f)\n",
    "    \n",
    "    dataset_meta['id'] = f'jonathanpick/{DATASET_NAME}'\n",
    "    dataset_meta['title'] = DATASET_NAME\n",
    "    \n",
    "    with open(f'/tmp/{DATASET_NAME}/dataset-metadata.json', 'w') as output:\n",
    "        json.dump(dataset_meta, output)\n",
    "    print(dataset_meta)\n",
    "    \n",
    "    !cp /tmp/{DATASET_NAME}/dataset-metadata.json /tmp/{DATASET_NAME}/meta.json\n",
    "    !ls /tmp/{DATASET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and write w multiprocessing\n",
    "N_FEATURES = 543\n",
    "\n",
    "def encode_example(sequence: np.ndarray, frame: np.ndarray, user_id: int):\n",
    "    feature = {\n",
    "        'user_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[user_id])),\n",
    "        'sequence': tf.train.Feature(bytes_list=tf.train.BytesList(value=[sequence.tobytes()])),\n",
    "        'frame': tf.train.Feature(bytes_list=tf.train.BytesList(value=[frame.tobytes()]))\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()    \n",
    "\n",
    "def decode_example(b):\n",
    "    features = {\n",
    "    'frame': tf.io.FixedLenFeature([], dtype=tf.dtypes.string),\n",
    "    'sequence': tf.io.FixedLenFeature([], dtype=tf.dtypes.string)\n",
    "    }\n",
    "    decoded = tf.io.parse_single_example(b, features)\n",
    "    frame = np.frombuffer(decoded['frame'].numpy()).reshape(-1, 543, 3)\n",
    "    sequence = np.frombuffer(decoded['sequence'].numpy(), dtype=np.int64)\n",
    "    return (sequence, frame)\n",
    "\n",
    "class ParquetReader():\n",
    "    \"\"\"Parallel-processing reader for lists of parquet files\"\"\"\n",
    "    def __init__(self, file_names):\n",
    "        self.file_names = file_names\n",
    "        self.current, self.next, self.index = pd.DataFrame([]), pd.DataFrame([]), pd.Index([])\n",
    "        self.current_path, self.next_path = None, None\n",
    "        \n",
    "    def _swap_parquet(self):\n",
    "        if self.current.empty:\n",
    "            self.current_path = self.file_names.pop()\n",
    "            self.current = pd.read_parquet(self.current_path)\n",
    "            self.index = self.current.index.unique()\n",
    "            if self.file_names:\n",
    "                self.next_path = self.file_names.pop()\n",
    "                self.next = pd.read_parquet(self.next_path) # TODO: eventually another process\n",
    "        elif not self.next.empty:\n",
    "            del self.current\n",
    "            self.current = self.next\n",
    "            self.index = self.current.index.unique()\n",
    "            self.current_path = self.next_path\n",
    "            if self.file_names:\n",
    "                self.next_path = self.file_names.pop()\n",
    "                self.next = pd.read_parquet(self.next_path)\n",
    "            else:\n",
    "                self.next = None\n",
    "        else:\n",
    "            return\n",
    "        self.progress = tqdm(desc=f'reader {self.current_path}', total=len(self.index))\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if idx not in self:\n",
    "            self._swap_parquet()\n",
    "            assert idx in self # it should always be the case that idx is now in the index\n",
    "        \n",
    "        frame = self.current.loc[idx]\n",
    "        self.progress.update(1)\n",
    "        \n",
    "        if type(frame) != pd.DataFrame:\n",
    "            return None\n",
    "        frame = frame.drop('frame', axis=1)\n",
    "        \n",
    "        frame = frame[sorted(frame.columns, key=lambda x: x[2:])] # group the columns by their feature in three coordinates (x,y,z)\n",
    "        n_frames = len(frame)\n",
    "        \n",
    "        return frame.values.reshape(n_frames, N_FEATURES, 3) # we shape this into a frame x feature x axis tensor\n",
    "    \n",
    "    def __contains__(self, idx):\n",
    "        if self.index.empty:\n",
    "            return False\n",
    "        else: return idx in self.index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "class ChunkIter():\n",
    "    \"\"\"Iterate through a list of chunk examples and produce coordinate matrices\"\"\"\n",
    "    def __init__(self, chunk_list, parquet_reader):\n",
    "        self.chunk_list = chunk_list\n",
    "        self.parquet_reader = parquet_reader\n",
    "\n",
    "    def __iter__(self):\n",
    "        for chunk in self.chunk_list:\n",
    "            chunk_seqs = []\n",
    "            chunk_frames = []\n",
    "            fold = chunk.iloc[0].fold\n",
    "            for idx in chunk.index:\n",
    "                example = chunk.loc[idx]\n",
    "                seq_id = example.sequence_id\n",
    "                seq = np.array(list(map(lambda x: WORD_DICT[x], example.phrase)))\n",
    "                frame = self.parquet_reader[seq_id]\n",
    "\n",
    "                chunk_frames += frame,\n",
    "                chunk_seqs += seq,\n",
    "            yield (chunk_frames, chunk_seqs), fold\n",
    "\n",
    "class ParallelFoldWriter():\n",
    "    \"\"\"A processing class that reads and writes tfrecord files in parallel. It is a little bit tedious because we don't have 1:1 correspondence between parquet files and training examples.\n",
    "    Yeah this might be a bit over-engineered. And yes, I know that it should use parallell processing instead of coroutines. That will be addressed when I am done with the project and have time to worry about these things.\"\"\"\n",
    "    def __init__(self, manifest: pd.DataFrame, read_dir: str, write_dir: str, n_folds: int, chunk_size: int, workers: int):\n",
    "        # generate fold groups\n",
    "        manifest = [df for _, df in manifest.groupby('file_id')]\n",
    "        random.shuffle(manifest)\n",
    "        manifest = pd.concat(manifest).reset_index()\n",
    "        fold_size = len(manifest) // n_folds\n",
    "        manifest['fold'] = manifest.index // chunk_size\n",
    "        file_groups = list(map(lambda x: os.path.join(read_dir, x), manifest['path'].unique()))[::-1]\n",
    "\n",
    "        # we will probably have a hanging portion in the last fold\n",
    "        if len(manifest) % n_folds:\n",
    "            manifest.loc[n_folds*fold_size:,'fold'] = n_folds - 1\n",
    "\n",
    "        self.chunks = [c for _, c in manifest.groupby(lambda x: x // chunk_size)]\n",
    "        self.write_dir = write_dir\n",
    "        self.reader = ParquetReader(file_groups)\n",
    "        self.workers = workers\n",
    "\n",
    "    def _write_chunk(self, data):\n",
    "        print('wrote a chunk')\n",
    "        chunk, fold = data\n",
    "        chunk_size = len(chunk)\n",
    "        filename = os.path.join(self.write_dir, f'fold{fold}-{chunk_num}-{chunk_size}.tfrecord')\n",
    "        options=tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "        writer = tf.io.TFRecordWriter(filename, options=options)\n",
    "        for sequence, frame in chunk:\n",
    "            encoded_bytes = encode_example(sequence, frame)\n",
    "            writer.write(encoded_bytes)\n",
    "        writer.close()\n",
    "        \n",
    "        \n",
    "    def do_writes(self):\n",
    "        chunk_iter = ChunkIter(self.chunks, self.reader)\n",
    "        process_chunks = map(lambda x: it.islice(chunk_iter, x, x+self.workers), range(0, len(self.chunks), self.workers))\n",
    "        for c in process_chunks:\n",
    "            for chunk in c:\n",
    "                p = multiprocessing.Process(target=self._write_chunk, args=chunk)\n",
    "                p.start()\n",
    "                break\n",
    "                \n",
    "            # process_pool = [multiprocessing.Process(target=self._write_chunk, args=(chunk, )).start() for chunk in c]\n",
    "            # for process in process_pool:\n",
    "                # process.join()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07137c3ad9144693aac34d049fcb95ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reader ./train_landmarks/1019715464.parquet:   0%|          | 0/998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_hashlib.HMAC' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m writer \u001b[38;5;241m=\u001b[39m ParallelFoldWriter(manifest, DATA_DIR, DATASET_DIR, N_FOLDS, CHUNK_SIZE, N_WORKERS)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_writes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 138\u001b[0m, in \u001b[0;36mParallelFoldWriter.do_writes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m c:\n\u001b[1;32m    137\u001b[0m     p \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mProcess(target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_chunk, args\u001b[38;5;241m=\u001b[39mchunk)\n\u001b[0;32m--> 138\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dl/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dl/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dl/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dl/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dl/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dl/lib/python3.11/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dl/lib/python3.11/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_hashlib.HMAC' object"
     ]
    }
   ],
   "source": [
    "writer = ParallelFoldWriter(manifest, DATA_DIR, DATASET_DIR, N_FOLDS, CHUNK_SIZE, N_WORKERS)\n",
    "writer.do_writes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/fold*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: move this to the colab notebook\n",
    "\n",
    "# # from IPython.display import HTML\n",
    "# import matplotlib.animation as animation\n",
    "# from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# def filter_nans(frames):\n",
    "#     return \n",
    "\n",
    "# def plot_frame(frame, edges=[], indices=[]):\n",
    "#     frame[np.isnan(frame)] = 0\n",
    "#     x = list(frame[...,0])\n",
    "#     y = list(frame[...,1])\n",
    "#     if len(indices) == 0:\n",
    "#         indices = list(range(len(x)))\n",
    "#     ax.clear()\n",
    "#     ax.scatter(x, y, color='blue')\n",
    "#     for i in range(len(x)):\n",
    "#         ax.text(x[i], y[i], indices[i])\n",
    "    \n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])\n",
    "#     ax.set_xticklabels([])\n",
    "#     ax.set_yticklabels([])\n",
    "    \n",
    "# def animate_frames(frames, edges=[], indices=[]):\n",
    "#         anim = FuncAnimation(fig, lambda frame: plot_frame(frame, edges, indices), frames=frames, interval=100)\n",
    "#         return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = f'ASLF-{n_splits}fold'\n",
    "\n",
    "os.makedirs(f'/tmp/{DATASET_NAME}', exist_ok=True)\n",
    "\n",
    "with open('/kaggle/input/kaggleapi/kaggle.json') as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "    \n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n",
    "\n",
    "!kaggle datasets init -p /tmp/{DATASET_NAME}\n",
    "\n",
    "with open(f'/tmp/{DATASET_NAME}/dataset-metadata.json') as f:\n",
    "    dataset_meta = json.load(f)\n",
    "\n",
    "dataset_meta['id'] = f'jonathanpick/{DATASET_NAME}'\n",
    "dataset_meta['title'] = DATASET_NAME\n",
    "\n",
    "with open(f'/tmp/{DATASET_NAME}/dataset-metadata.json', 'w') as output:\n",
    "    json.dump(dataset_meta, output)\n",
    "print(dataset_meta)\n",
    "\n",
    "!cp /tmp/{DATASET_NAME}/dataset-metadata.json /tmp/{DATASET_NAME}/meta.json\n",
    "!ls /tmp/{DATASET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeGroup = ParallelFoldWriterGroup(manifest, f'/tmp/{DATASET_NAME}', 5, 256, 10)\n",
    "await writeGroup.main_write_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "version_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "settings = {\n",
    "    'id': 'id',\n",
    "    'secret': 'secret'\n",
    "}\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=settings['id'],\n",
    "    aws_secret_access_key=settings['secret'],\n",
    "    region_name='us-west-1'\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = DATASET_NAME.lower() + \"-\" + version_name\n",
    "bucket = s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={\n",
    "                'LocationConstraint': 'us-west-1'\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_directory(bucket, directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            bucket.upload_file(os.path.join(root, file), file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_directory(bucket, f'/tmp/{DATASET_NAME}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
